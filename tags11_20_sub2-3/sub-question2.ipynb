{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c49faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2021/11/25 12:40\n",
    "# @Author  : hit-itnlp-fengmq\n",
    "# @FileName: sub2_question_concat.py\n",
    "# @Software: PyCharm\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from collections import namedtuple\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "setup_seed(2021)\n",
    "\n",
    "\n",
    "# 加载模型\n",
    "def get_premodel(path=r\"D:\\Anaconda\\learn\\_Bert\\pre_train_model\\albert-base-v2\"):\n",
    "    '''\n",
    "    :param path: 预训练模型在本机的路径\n",
    "    :return:\n",
    "    '''\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(path)  # Embedding(30000, 128) [MASK]:4\n",
    "    print(\"model load end!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def scores(question: str, text: list, model, top_k: int = 3):\n",
    "    text_embeddings = model.encode(text, convert_to_tensor=True)\n",
    "    query_embedding = model.encode(question, convert_to_tensor=True)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        text_embeddings.to('cuda')\n",
    "        query_embedding = torch.unsqueeze(query_embedding, dim=0)\n",
    "        query_embedding.to('cuda')\n",
    "        text_embeddings = util.normalize_embeddings(text_embeddings)\n",
    "        query_embedding = util.normalize_embeddings(query_embedding)\n",
    "    top_results = util.semantic_search(query_embedding, text_embeddings\n",
    "                                       , top_k=top_k\n",
    "                                       )  # [[{corpus_id:,score:},{},{}]]\n",
    "    text_ids = [item['corpus_id'] for item in top_results[0]]\n",
    "    # 还原顺序\n",
    "    text_ids.sort()\n",
    "    result = []\n",
    "    for id in text_ids:\n",
    "        result.append(text[id])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06985144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(path=r'2_all_data.json', is_topk=False, Sbert=None, fold=0):\n",
    "    f = open(path, 'r', encoding='utf-8')\n",
    "    json_data = json.load(f)\n",
    "    f.close()\n",
    "    print(\"split data begin !\")\n",
    "    datas = []\n",
    "    for items in tqdm(json_data):\n",
    "        list_qa = items['list_2qa']\n",
    "        text_tags = items['text_tags']\n",
    "        all_recipe_texts = [i['text'] for i in text_tags]  # 从question_tag得到所有的文本信息。\n",
    "        for qa in list_qa:\n",
    "            ques, answer = qa['quest'], qa['answer']\n",
    "\n",
    "            if is_topk and Sbert:  # 利用sentence Bert提取前k句话\n",
    "                sim_texts = scores(ques, all_recipe_texts, Sbert)  # 使用sentence Bert得到最相似的k句话\n",
    "                sim_texts_tags = []  # 存储sim_text和它对应的tags\n",
    "                for st in sim_texts:  # 将每一句得到tag\n",
    "                    for tts in text_tags:\n",
    "                        if tts['text'] == st:\n",
    "                            sim_texts_tags.append(tts)\n",
    "                            break\n",
    "                sub_item_dict = {}\n",
    "                sub_item_dict[\"ques\"] = ques\n",
    "                sub_item_dict[\"answer\"] = answer\n",
    "                sub_item_dict[\"sim_texts\"] = sim_texts_tags\n",
    "                datas.append(sub_item_dict)\n",
    "            else:\n",
    "                sub_item_dict = {}\n",
    "                sub_item_dict[\"ques\"] = ques\n",
    "                sub_item_dict[\"answer\"] = answer\n",
    "                sub_item_dict[\"sim_texts\"] = text_tags\n",
    "                datas.append(sub_item_dict)\n",
    "        break\n",
    "    num_data = len(datas)\n",
    "    num_test = num_data // 5\n",
    "    random.shuffle(datas)\n",
    "\n",
    "    test = datas[fold * num_test:(fold + 1) * num_test]\n",
    "    if fold == 0:\n",
    "        train = datas[num_test:]\n",
    "    else:\n",
    "        train = datas[:num_test * fold]\n",
    "        train.extend(datas[num_test * (fold + 1):])\n",
    "    print(\"split data end !\")\n",
    "\n",
    "    print(\"nums of train data:{}\".format(len(train)))\n",
    "    print(\"nums of val data:{}\".format(len(test)))\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def pre_process(data):\n",
    "    '''\n",
    "    将tag进行处理，例如Tool=spatula.2.3.1，直接处理为spatula，\n",
    "    但是若同时存在Habitat ,Tool，会取优先出现的，\n",
    "    同时过滤tag全部为_的句子，\n",
    "    '''\n",
    "\n",
    "    def useful_tag(tags: str):\n",
    "        tags = tags.lower()\n",
    "        if \"|\" in tags:\n",
    "            tags = tags.split(\"|\")\n",
    "            ans=[]\n",
    "            for tag in tags:\n",
    "                if 'habitat' in tag or 'tool' in tag:\n",
    "                    name,tag =  tag.split(\"=\")   \n",
    "                    tag = tag.split(\".\")[0]\n",
    "                    \n",
    "                    if '_' in tag:  # medium_oven_-_proof_skillet\n",
    "                        tag = tag.split(\"_\")\n",
    "                        tag = \" \".join(tag)\n",
    "                    ans.append(name+\" : \"+tag)\n",
    "            if len(ans)!=0:\n",
    "                return ans\n",
    "            return \"_\"\n",
    "        else:\n",
    "            ans=[]\n",
    "            if 'habitat' in tags or 'tool' in tags:\n",
    "                name,tag =  tags.split(\"=\")   \n",
    "                tag = tag.split(\".\")[0]\n",
    "                if '_' in tag:  # medium_oven_-_proof_skillet\n",
    "                    tag = tag.split(\"_\")\n",
    "                    tag = \" \".join(tag)\n",
    "                ans.append(name+\" : \"+tag)\n",
    "                return ans\n",
    "            else:\n",
    "                return \"_\"\n",
    "\n",
    "    print(\"pre_process data start !\")\n",
    "    # Habitat ,Tool\n",
    "    new_data = []\n",
    "    for item in data:\n",
    "        sim_texts = item['sim_texts']\n",
    "        new_sim_texts = []  # 更换sim_texts\n",
    "        for i in sim_texts:\n",
    "            text = i['text'].lower()\n",
    "            tags = i['tags']  # 第二类问题只有一个tags\n",
    "            new_tags = []\n",
    "            for tg in tags:\n",
    "                if tg == \"_\":\n",
    "                    new_tags.append(\"_\")\n",
    "                else:\n",
    "                    t_tg = useful_tag(tg)\n",
    "                    if t_tg!=\"_\":\n",
    "                        temp = \"# \"+(\" # \".join(t_tg))+\" #\"\n",
    "                    else:\n",
    "                        temp = (\" # \".join(t_tg))\n",
    "                    new_tags.append(temp)\n",
    "            if new_tags.count(\"_\") == len(new_tags):  # 说明tag全部为_，直接过滤掉\n",
    "                continue\n",
    "            temp_dict = {}\n",
    "            temp_dict['text'] = text\n",
    "            temp_dict['tags'] = new_tags\n",
    "            new_sim_texts.append(temp_dict)\n",
    "\n",
    "        item[\"sim_texts\"] = new_sim_texts\n",
    "        new_data.append(item)\n",
    "    print(\"pre_process data end !\")\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b97ed453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):  # 需要继承data.Dataset\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        q = item['ques'].strip(\" \").lower()\n",
    "        a = item['answer'].strip(\" \")\n",
    "        texts = item['sim_texts']\n",
    "        # 处理答案，分离两种\n",
    "        if \"by using a\" in a:\n",
    "            a = a.split(\" \")[3:]\n",
    "            a = \" \".join(a)\n",
    "        print(q)\n",
    "        print(a)\n",
    "        # 处理text和tag\n",
    "        text,tag=[],[]\n",
    "        for item_text in texts:\n",
    "            text += item_text['text'].strip(\" \").split(\" \")\n",
    "            tag += item_text['tags']\n",
    "        if len(text)!=len(tag):\n",
    "            print(\"length not equal!\")\n",
    "        new_text = []\n",
    "        for i in range(len(text)):\n",
    "            if tag[i]==\"_\":\n",
    "                new_text.append(text[i])\n",
    "            else:\n",
    "                new_text.append(text[i]+\" \"+tag[i])\n",
    "        text = \" \".join(new_text)\n",
    "        print(text)\n",
    "        ##q,a,text,tag\n",
    "        # encode= self.tokenizer(q, text+tag, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        encode = self.tokenizer.encode_plus(q,\n",
    "                                            text,\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length=512,\n",
    "                                            padding='max_length',\n",
    "                                            return_attention_mask=True,\n",
    "                                            return_tensors='pt',\n",
    "                                            truncation=True)\n",
    "        input_ids, token_type_ids, attention_mask = encode['input_ids'], encode['token_type_ids'], encode[\n",
    "            'attention_mask']\n",
    "        # 获取起始位置\n",
    "        start, end = self.start_end(a, q, text)\n",
    "        return input_ids.squeeze(), token_type_ids.squeeze(), attention_mask.squeeze(), torch.tensor(\n",
    "            start), torch.tensor(end)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def start_end(self, answer, q, text):\n",
    "        # 有问题\n",
    "        answer_encode = self.tokenizer(answer.strip().lower())['input_ids'][1:-1]\n",
    "        text_encode = self.tokenizer(q.strip().lower(), text.strip().lower())['input_ids']\n",
    "        start_end = ()\n",
    "        for i in range(len(text_encode)):\n",
    "            if text_encode[i] == answer_encode[0]:\n",
    "                j = 0\n",
    "                for j in range(len(answer_encode)):\n",
    "                    if text_encode[i + j] != answer_encode[j]:\n",
    "                        break\n",
    "                if j == len(answer_encode) - 1:\n",
    "                    if text_encode[i + j] != answer_encode[j]:\n",
    "                        continue\n",
    "                    start_end = (i, i + len(answer_encode) - 1)\n",
    "            if len(start_end) != 0:\n",
    "                return start_end\n",
    "        if len(start_end) == 0:\n",
    "            start_end = (0, 0)\n",
    "        return start_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b8a68cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model load end!\n",
      "split data begin !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split data end !\n",
      "nums of train data:4\n",
      "nums of val data:0\n",
      "pre_process data start !\n",
      "pre_process data end !\n",
      "pre_process data start !\n",
      "pre_process data end !\n",
      "how do you cut the broccoli and stem?\n",
      "knife\n",
      "cut # tool : knife # the broccoli into flowerets and the stem into bite - size pieces ; cut # tool : knife # carrots and zucchini into cubes . add the tinned tomatoes to the cooked vegetables , mash # tool : spoon # them coarsely , stir # tool : spoon # and combine .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([    2,   184,   107,    42,  1077,    14,   334, 23258,  1210,    17,\n",
       "          6940,    60,     3,  1077,  6926,  5607,    13,    45,  4026,  6926,\n",
       "            14,   334, 23258,  1210,    77,  5383,  6095,    17,    14,  6940,\n",
       "            77,  5804,    13,     8,  1072,  2491,    13,    73,  1077,  6926,\n",
       "          5607,    13,    45,  4026,  6926, 21555,    18,    17,  6308,  3384,\n",
       "         25442,    77, 13682,    18,    13,     9,  3547,    14,  5353,  3725,\n",
       "         14200,    20,    14, 15853, 12960,    13,    15, 13333,  6926,  5607,\n",
       "            13,    45, 12153,  6926,   105, 19809,   102,    13,    15, 13216,\n",
       "          6926,  5607,    13,    45, 12153,  6926,    17, 12287,    13,     9,\n",
       "             3,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor(18),\n",
       " tensor(18))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer = get_premodel()\n",
    "\n",
    "Sbert = SentenceTransformer(r'D:\\Anaconda\\learn\\_Bert\\pre_train_model\\all-MiniLM-L6-v2')\n",
    "train_data, test_data = split_data(is_topk=True,Sbert = Sbert)\n",
    "train_data, test_data = pre_process(train_data), pre_process(test_data)\n",
    "myDataset(train_data,tokenizer)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37dceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, albert, QAhead):\n",
    "        super().__init__()\n",
    "        self.albert = albert\n",
    "\n",
    "        self.qa_outputs = QAhead  # self.qa_outputs = nn.Linear(768, 2,bias=True)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, start_positions=None, end_positions=None):\n",
    "        albert_output = self.albert(input_ids=input_ids,\n",
    "                                    token_type_ids=token_type_ids,\n",
    "                                    attention_mask=attention_mask)\n",
    "        last_hidden_state = albert_output.last_hidden_state  # torch.Size([batchSize, 512, 768]) 512与max_length相关\n",
    "        logits = self.qa_outputs(last_hidden_state)  # torch.Size([batchSize, 512, 2])\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)  # 分离出的start_logits/end_logits形状为([batchSize, 512, 1])\n",
    "        start_logits = start_logits.squeeze(-1)  # ([batchSize, 512])\n",
    "        end_logits = end_logits.squeeze(-1)  # ([batchSize, 512])\n",
    "\n",
    "        Outputs = namedtuple('Outputs', 'start_logits, end_logits')\n",
    "        outputs = Outputs(start_logits, end_logits)\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "            Outputs = namedtuple('Outputs', 'loss,start_logits, end_logits')\n",
    "            outputs = Outputs(total_loss, start_logits, end_logits)\n",
    "        return outputs\n",
    "\n",
    "def train(model, train_dataloader, testdataloader, device, fold=0, epoch=5):\n",
    "    model.to(device)\n",
    "    optim = AdamW(model.parameters(), lr=1e-5, weight_decay=0.2)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optim, num_warmup_steps=400, num_training_steps=len(train_dataloader) * epoch)\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        model.train()\n",
    "        train_acc2 = []\n",
    "        train_loss = []\n",
    "        loop = tqdm(train_dataloader, leave=True)\n",
    "        for data in tqdm(loop):\n",
    "            optim.zero_grad()\n",
    "            data = tuple(t.to(device) for t in data)\n",
    "            input_ids, token_type_ids, attention_mask, start, end = data\n",
    "\n",
    "            outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
    "                            start_positions=start, end_positions=end)\n",
    "            loss, start_logits, end_logits = outputs.loss, outputs.start_logits, outputs.end_logits\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            start_pred = torch.argmax(start_logits, dim=1)\n",
    "            end_pred = torch.argmax(end_logits, dim=1)\n",
    "\n",
    "            acc = ((start_pred == start).sum() / len(start_pred)).item()\n",
    "            starts = (np.array((start_pred == start).cpu()))\n",
    "            ends = (np.array((end_pred == end).cpu()))\n",
    "            acc2 = np.all(np.array([starts, ends]).T, axis=-1).astype(int)\n",
    "            train_acc2.extend(acc2)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            loop.set_description(f'fold:{fold}  Epoch:{epoch}')\n",
    "            loop.set_postfix(loss=loss.item(), acc=acc)\n",
    "        # if epoch >=3:\n",
    "        #     model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        #     model_path = r\"fold\" + str(fold) + \"_epoch\" + str(epoch)\n",
    "        #     if not os.path.exists(model_path):\n",
    "        #         os.makedirs(model_path)\n",
    "        #     torch.save(model_to_save, os.path.join(model_path,'model.pt'))\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        test_acc2 = []\n",
    "        for data in tqdm(testdataloader):\n",
    "            with torch.no_grad():\n",
    "                data = tuple(t.to(device) for t in data)\n",
    "                input_ids, token_type_ids, attention_mask, start, end = data\n",
    "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
    "                               start_positions=start, end_positions=end)\n",
    "\n",
    "                loss = outputs.loss\n",
    "                start_pred = torch.argmax(outputs.start_logits, dim=1)\n",
    "                end_pred = torch.argmax(outputs.end_logits, dim=1)\n",
    "\n",
    "\n",
    "                starts = (np.array((start_pred == start).cpu()))\n",
    "                ends = (np.array((end_pred == end).cpu()))\n",
    "                acc2 = np.all(np.array([starts, ends]).T, axis=-1).astype(int)\n",
    "                test_acc2.extend(acc2)\n",
    "                test_loss.append(loss.item())\n",
    "        print(\"{}, Train_acc2:{} Train_loss:{}-----Val_acc:{}  Val_loss:{}\".format(\n",
    "            epoch,  np.mean(train_acc2), np.mean(train_loss), np.mean(test_acc2),\n",
    "            np.mean(test_loss)))\n",
    "\n",
    "\n",
    "for fold in range(1):\n",
    "    #     # 修改模型路径，以及对应的QAhead.pickle。根据设备酌情修改batchsize\n",
    "    model, tokenizer = get_premodel(r'/home/mqfeng/R2QA/pretrain_recipeQA/large/epoch2')\n",
    "    with open(r'QAhead_large.pickle', 'rb') as file:\n",
    "        QAhead = pickle.load(file)\n",
    "    myModel = MyModel(model.albert, QAhead)\n",
    "    Sbert = SentenceTransformer(r'/home/mqfeng/preModels/all-MiniLM-L6-v2')\n",
    "    train_data, test_data = split_data(is_topk=True,Sbert = Sbert)\n",
    "    train_data, test_data = pre_process(train_data), pre_process(test_data)\n",
    "\n",
    "    # 构造DataSet和DataLoader\n",
    "    train_Dataset = myDataset(train_data, tokenizer)\n",
    "    test_Dataset = myDataset(test_data, tokenizer)\n",
    "    # 修改batchsize\n",
    "    train_Dataloader = DataLoader(train_Dataset, batch_size=4, shuffle=True)\n",
    "    test_Dataloader = DataLoader(test_Dataset, batch_size=1)\n",
    "    # 训练\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = torch.device('cpu')\n",
    "    train(myModel,train_Dataloader, test_Dataloader, device, fold, epoch=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
