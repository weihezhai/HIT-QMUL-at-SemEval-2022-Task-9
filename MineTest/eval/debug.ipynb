{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31e662e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2022/1/4 19:00\n",
    "# @Author  : hit-itnlp-fengmq\n",
    "# @FileName: eval.py\n",
    "# @Software: PyCharm\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "# from sentence_transformers import util, SentenceTransformer\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "def setup_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "setup_seed(21)\n",
    "\n",
    "\n",
    "# 加载模型\n",
    "def get_premodel(path=r\"D:\\Anaconda\\learn\\_Bert\\pre_train_model\\t5-small\"):\n",
    "    '''\n",
    "    :param path: 预训练模型在本机的路径\n",
    "    :return:\n",
    "    '''\n",
    "    tokenizer = T5Tokenizer.from_pretrained(path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(path)\n",
    "    print(\"model load end!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "class myDataset(Dataset):  # 需要继承data.Dataset\n",
    "    def __init__(self, data_text, data_qa, tokenizer):\n",
    "        self.data_text = data_text\n",
    "        self.data_qa = data_qa\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.max_source_length = 1024\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data_qa[index]\n",
    "        question = item['question']\n",
    "        doc_id = item['doc_id'].strip()\n",
    "        q_id = item['q_id']\n",
    "\n",
    "        texts = self.data_text[doc_id]\n",
    "        text = \"\".join(texts)\n",
    "\n",
    "        q_text = question + \" : \" + text\n",
    "        source_encoding = self.tokenizer.encode_plus(q_text,\n",
    "                                                     add_special_tokens=True,\n",
    "                                                     max_length=self.max_source_length,\n",
    "                                                     padding='max_length',\n",
    "                                                     return_attention_mask=True,\n",
    "                                                     return_tensors='pt',\n",
    "                                                     truncation=True)\n",
    "        input_ids, attention_mask = source_encoding['input_ids'], source_encoding['attention_mask']\n",
    "        return input_ids.squeeze(), doc_id, q_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_qa)\n",
    "\n",
    "\n",
    "def get_dataloader(tokenizer, qa_path=\"test_qa.json\", text_path=r\"test3.json\", batchsize=4):\n",
    "    data_text = json.load(open(text_path, 'r', encoding='utf-8'))\n",
    "    data_qa = json.load(open(qa_path, 'r', encoding='utf-8'))\n",
    "    dataset = myDataset(data_text, data_qa, tokenizer)\n",
    "\n",
    "    batch_size = batchsize\n",
    "    dataloader = DataLoader(\n",
    "        dataset,  # The training samples.\n",
    "        sampler=SequentialSampler(dataset),  # Select batches randomly\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    print(\"dataloader load end!\")\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def eval(model, tokenizer, test_dataloader):\n",
    "    result = {}\n",
    "\n",
    "    device = torch.device( 'cpu')#'cuda' if torch.cuda.is_available() else\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_dataloader):\n",
    "            input_ids, doc_id, q_id = data\n",
    "            doc_id, q_id=doc_id[0], q_id[0]\n",
    "            input_ids = input_ids.to(device)\n",
    "            outputs = model.generate(input_ids)\n",
    "            decode_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(input_ids)\n",
    "            print(doc_id)\n",
    "            print(q_id)\n",
    "            print(decode_output)\n",
    "            if doc_id not in result.keys():\n",
    "                result[doc_id] = {}\n",
    "            result[doc_id][q_id] = decode_output\n",
    "            break\n",
    "    w = open(\"r2vq_pred.json\", \"w\", encoding=\"utf-8\")\n",
    "    json.dump(result, w, ensure_ascii=False, indent=4)\n",
    "    w.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a92fdd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model load end!\n",
      "dataloader load end!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1721 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  213,   225,    25,  ...,     0,     0,     0],\n",
      "        [  149,   186, 14987,  ...,     0,     0,     0]])\n",
      "r-378\n",
      "2-3\n",
      "#, drain # drop : eggplant, habitat : sink # any excess water\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# eval的时候只能设置为1\n",
    "batchsize = 2\n",
    "model, tokenizer = get_premodel()\n",
    "test_dataloader = get_dataloader(tokenizer, qa_path=\"test_qa.json\", text_path=r\"test3.json\", batchsize=batchsize)\n",
    "eval(model, tokenizer, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3035818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
