{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01dc5dc3",
   "metadata": {},
   "source": [
    "# extract type 0 data only\n",
    "## and removed answers with magnitude more than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3424be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def extract_id(start_token='0', filepath=r'alldata.json'):\n",
    "    start = start_token\n",
    "    texts = []\n",
    "    with open(filepath,\"r\",encoding='utf-8') as f:\n",
    "        data=json.load(f)\n",
    "    for key,item in data.items():\n",
    "        questions=item['question']\n",
    "        for q in questions:\n",
    "            q_temp,a=q.split('     ')\n",
    "            idx,q = q_temp.split(\"=\")\n",
    "            idx_=idx.split(\" \")[-2]\n",
    "            if idx_.startswith(start) and int(a)<=3:\n",
    "                text={}\n",
    "                text['q_id']=idx_\n",
    "                text['question']=q.strip()\n",
    "                text['answer']=a\n",
    "                text['doc_id']=key.strip()\n",
    "                texts.append(text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c816a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract type 0 qa pairs\n",
    "fp = '/Users/rickzhai/Documents/GitHub/ITNLP_Semeval2022_Task6/allqapairs.json'\n",
    "t = extract_id(start_token='0', filepath=fp)\n",
    "write = open(\"qa_0.json\", \"w\", encoding=\"utf-8\")\n",
    "json.dump(t, write, ensure_ascii=False, indent=4)\n",
    "write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3598867",
   "metadata": {},
   "source": [
    "# create df as type_0 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3544f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "qa = r\"/Users/rickzhai/Documents/GitHub/ITNLP_Semeval2022_Task6/number_reasoning/qa_0.json\"\n",
    "recipe = r\"/Users/rickzhai/Documents/GitHub/ITNLP_Semeval2022_Task6/number_reasoning/padded_text_0_1.json\"\n",
    "with open(qa,\"r\",encoding='utf-8') as f:\n",
    "    qa_0=json.load(f)\n",
    "with open(recipe,\"r\",encoding='utf-8') as f:\n",
    "    c=json.load(f)\n",
    "data = {}\n",
    "data['question'] = []\n",
    "data['answer'] = []\n",
    "data['context'] = []\n",
    "for _ in qa_0:\n",
    "    if _.get('question').startswith('How many actions does it take') or _.get('question').startswith('How many times'):\n",
    "        data['question'].append(_.get('question'))\n",
    "        data['answer'].append(int(_.get('answer')))\n",
    "        docid = _.get('doc_id')\n",
    "        the_doc = next((sub for sub in c if sub['doc_id'] == docid), None)\n",
    "        context = ''.join(the_doc['text'])\n",
    "        data['context'].append(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ca817",
   "metadata": {},
   "source": [
    "# write to csv for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d541e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_csv('qa_0.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "863baace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               question  answer  \\\n",
      "0     How many actions does it take to process the m...       1   \n",
      "1     How many actions does it take to process the m...       1   \n",
      "2     How many actions does it take to process the o...       1   \n",
      "3     How many actions does it take to process the b...       2   \n",
      "4     How many actions does it take to process the w...       1   \n",
      "...                                                 ...     ...   \n",
      "1701  How many actions does it take to process the b...       1   \n",
      "1702  How many actions does it take to process the m...       1   \n",
      "1703                  How many times is the knife used?       2   \n",
      "1704  How many actions does it take to process the c...       1   \n",
      "1705                    How many times is the lid used?       1   \n",
      "\n",
      "                                                context  \n",
      "0     cut # result : chopped vegetables, tool : knif...  \n",
      "1     cut # result : chopped vegetables, tool : knif...  \n",
      "2     preheat the oven # part : preheat # to 350deg ...  \n",
      "3     preheat the oven # part : preheat # to 350deg ...  \n",
      "4     cook pasta # part : cook # in large salted # p...  \n",
      "...                                                 ...  \n",
      "1701  finely dice # tool : knife, habitat : cutting ...  \n",
      "1702  finely dice # tool : knife, habitat : cutting ...  \n",
      "1703  finely dice # tool : knife, habitat : cutting ...  \n",
      "1704  for cocoa mix , in a storage # part : combine ...  \n",
      "1705  for cocoa mix , in a storage # part : combine ...  \n",
      "\n",
      "[1706 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9040b3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-macosx_10_6_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7db3ddbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ALBERT tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38a3fb0ee224fcc945dc55116d77429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b90e0266ab44df985a9282b03c968dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381365e6d03f4f0e9306eec8d2cb55ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120a9231cafb4091b725da4456756df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/866 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LongformerTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading ALBERT tokenizer...')\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-large-4096-finetuned-triviaqa', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcec816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  880\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./qa_0.csv')\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in df.context.values:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53c211fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/rickzhai/opt/anaconda3/lib/python3.8/site-packages (1.10.0)\n",
      "Requirement already satisfied: typing_extensions in /Users/rickzhai/opt/anaconda3/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8050f7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,  6179,   171,  2163,   473,    24,   185,     7,   609,     5,\n",
      "        37570,  4884,   116,     2,     2,  8267,   849,   898,  4832, 16639,\n",
      "         8942,     6,  3944,  4832,  7023,   849,     5, 34803,   849,   233,\n",
      "         4832,   847,   849,    88,  3041,  2816,  1872,     8,     5, 10114,\n",
      "          849,   233,  4832,   847,   849,    88, 10970,   111,  1836,  3745,\n",
      "        25606,   847,   849,  3944,  4832,  7023,   849, 28488,   849,   233,\n",
      "         4832,   847,   849,     8,   992,  3964, 33764,   849,   233,  4832,\n",
      "          847,   849,    88, 35788,   479,  2241,  4467,   849, 14294,  4832,\n",
      "         5730,   849, 19543,   849,   233,  4832,  2241,  4467,   849,    11,\n",
      "          132, 25119,     9, 14983,   849,   233,  4832,  2241,  4467,   849,\n",
      "          681,  2156,  1606,   849,   898,  4832, 14166,  8942,     6, 14294,\n",
      "         4832,  5730,   849, 16639,   849,   233,  4832,  1606,   849,  8942,\n",
      "            8,  7142,    13,   158,   728,    81,   614,  2859,  2156, 21881,\n",
      "        10930,   479,    11,    10,  2559,  5730,   849,   233,  4832,  2241,\n",
      "         4467,   849,  2241,  4467, 37570,   849,   233,  4832,  2241,  4467,\n",
      "          849,  4884,  3433,   849,  3944,  4832, 20231,  5571,   849,    24,\n",
      "           62,   157,  2156,     8, 14351,   849,  3944,  4832, 20231,  5571,\n",
      "          849,    13,   231,   111,   290,   728,   454,  6219,   196,   479,\n",
      "         1606,     5, 12899,  7456,   849,   233,  4832,  1606,   849, 18553,\n",
      "            7,     5, 14166,  8942,  2156, 29516,   849,  3944,  4832, 25409,\n",
      "          849,   106, 38667,   352,  2156, 14351,   849,  3944,  4832, 25409,\n",
      "          849,     8,  9637,   479,  2937,   849,   898,  4832,  8942,     8,\n",
      "        37570,  4884,   740, 16151,  4104,   849,     5,  4884,   849,   233,\n",
      "         4832,  2937,   849,    88,     5,  5730,    19,     5,  8942,  2156,\n",
      "          191,    19,  6740,   849,   233,  4832,   191,   849,     8, 10702,\n",
      "          849,   233,  4832,   191,   849,  2156,  3344, 12826,     7,  9637,\n",
      "           70,  7075,   849,   233,  4832,  9637,   849,   479,  1807,   849,\n",
      "         1874,  4832,  8942])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "sentences = df.context.values\n",
    "qs = df.question.values\n",
    "# For every sentence...\n",
    "for sent in zip(qs,sentences):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent[0],\n",
    "                        sent[1],                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                                  # Pad & truncate all sentences.\n",
    "                                   # Pad & truncate all sentences.\n",
    "                        max_length = 1024,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "#labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "#print('Original: ', sentences[0])\n",
    "#print(input_ids[0][:273])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7695204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,  6179,   171,  2163,   473,    24,   185,     7,   609,     5,\n",
      "        37570,  4884,   116,     2,     2,  8267,   849,   898,  4832, 16639,\n",
      "         8942,     6,  3944,  4832,  7023,   849,     5, 34803,   849,   233,\n",
      "         4832,   847,   849,    88,  3041,  2816,  1872,     8,     5, 10114,\n",
      "          849,   233,  4832,   847,   849,    88, 10970,   111,  1836,  3745,\n",
      "        25606,   847,   849,  3944,  4832,  7023,   849, 28488,   849,   233,\n",
      "         4832,   847,   849,     8,   992,  3964, 33764,   849,   233,  4832,\n",
      "          847,   849,    88, 35788,   479,  2241,  4467,   849, 14294,  4832,\n",
      "         5730,   849, 19543,   849,   233,  4832,  2241,  4467,   849,    11,\n",
      "          132, 25119,     9, 14983,   849,   233,  4832,  2241,  4467,   849,\n",
      "          681,  2156,  1606,   849,   898,  4832, 14166,  8942,     6, 14294,\n",
      "         4832,  5730,   849, 16639,   849,   233,  4832,  1606,   849,  8942,\n",
      "            8,  7142,    13,   158,   728,    81,   614,  2859,  2156, 21881,\n",
      "        10930,   479,    11,    10,  2559,  5730,   849,   233,  4832,  2241,\n",
      "         4467,   849,  2241,  4467, 37570,   849,   233,  4832,  2241,  4467,\n",
      "          849,  4884,  3433,   849,  3944,  4832, 20231,  5571,   849,    24,\n",
      "           62,   157,  2156,     8, 14351,   849,  3944,  4832, 20231,  5571,\n",
      "          849,    13,   231,   111,   290,   728,   454,  6219,   196,   479,\n",
      "         1606,     5, 12899,  7456,   849,   233,  4832,  1606,   849, 18553,\n",
      "            7,     5, 14166,  8942,  2156, 29516,   849,  3944,  4832, 25409,\n",
      "          849,   106, 38667,   352,  2156, 14351,   849,  3944,  4832, 25409,\n",
      "          849,     8,  9637,   479,  2937,   849,   898,  4832,  8942,     8,\n",
      "        37570,  4884,   740, 16151,  4104,   849,     5,  4884,   849,   233,\n",
      "         4832,  2937,   849,    88,     5,  5730,    19,     5,  8942,  2156,\n",
      "          191,    19,  6740,   849,   233,  4832,   191,   849,     8, 10702,\n",
      "          849,   233,  4832,   191,   849,  2156,  3344, 12826,     7,  9637,\n",
      "           70,  7075,   849,   233,  4832,  9637,   849,   479,  1807,   849,\n",
      "         1874,  4832,  8942,     8, 37570,  4884,   740, 16151,  4104,   849,\n",
      "           19, 14166,   849,   233,  4832,  1807,   849,  7666,    50,   402,\n",
      "         1122,    36, 46411,   438,  1827,  2156, 35455, 15042,  4839,   479,\n",
      "         1437,     2,     1,     1,     1,     1,     1,     1,     1,     1])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids[0][:310])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bfe3c1",
   "metadata": {},
   "source": [
    "# extract text & tag columns.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2cf67e1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile('[^a-zA-Z]') # to remove .1.2.3 in tags\n",
    "    \n",
    "with open(r\"/Users/rickzhai/Documents/GitHub/ITNLP_Semeval2022_Task6/crl_srl.csv\", 'r', encoding='utf-8') as f:\n",
    "    all_data = f.readlines()\n",
    "all_text_tag = []  # store all text tag columns\n",
    "i = 1\n",
    "while i < 219124:\n",
    "    doc_id = all_data[i - 1].split(\"=\")[1].strip(\"\\n\")  \n",
    "    text_tags = []  \n",
    "    while i < 219124 and 'newdoc id' not in all_data[i]:\n",
    "        item = all_data[i]\n",
    "        if i+1 < 219124:\n",
    "            item_next = all_data[i + 1] \n",
    "        \n",
    "        if 'text =' in item:\n",
    "            item_pre = all_data[i - 1]\n",
    "            if 'ingredients' in item_pre:\n",
    "                i += 1\n",
    "                pass\n",
    "            else:\n",
    "                i += 1\n",
    "                temp_text_tags = {}\n",
    "                texts, tagcols = \"\", {}\n",
    "\n",
    "                tagcols['word'] = []\n",
    "                tagcols['lemma'] = []\n",
    "                tagcols['pos'] = []\n",
    "                tagcols['entity'] = []\n",
    "                tagcols['part'] = []\n",
    "                tagcols['result'] = []\n",
    "                tagcols['hidden'] = []\n",
    "                tagcols['coref'] = []\n",
    "                tagcols['prdct'] = []\n",
    "                tagcols['arg1'] = []\n",
    "                tagcols['arg2'] = []\n",
    "                tagcols['arg3'] = []\n",
    "                tagcols['arg4'] = []\n",
    "                tagcols['arg5'] = []\n",
    "                tagcols['arg6'] = []\n",
    "                tagcols['arg7'] = []\n",
    "                tagcols['arg8'] = []\n",
    "                tagcols['arg9'] = []\n",
    "                tagcols['arg10'] = []\n",
    "                \n",
    "\n",
    "                while i < 219125 and all_data[i] and \"sent_id\" not in all_data[i] and 'newdoc id' not in all_data[i]: \n",
    "                    if \"newpar id\" in all_data[i]:\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    item = all_data[i]\n",
    "\n",
    "                    tags = item.strip().split(\"\\t\")\n",
    "                    if len(tags) != 1:\n",
    "\n",
    "                        tagcols['word'].append(tags[1])\n",
    "                        tagcols['lemma'].append(tags[2])\n",
    "                        tagcols['pos'].append(tags[3])\n",
    "                        tagcols['entity'].append(tags[4])\n",
    "                        tagcols['part'].append(tags[5])\n",
    "                        tagcols['result'].append(tags[6])\n",
    "                        tagcols['hidden'].append(useful_tag_8(tags[7])) # list of hidden tags ['shadow : water', 'habitat : pot']\n",
    "                        tagcols['coref'].append(tags[8])\n",
    "                        tagcols['prdct'].append(tags[9])\n",
    "                        tagcols['arg1'].append(tags[10])\n",
    "                        tagcols['arg2'].append(tags[11])\n",
    "                        tagcols['arg3'].append(tags[12])\n",
    "                        tagcols['arg4'].append(tags[13])\n",
    "                        tagcols['arg5'].append(tags[14])\n",
    "                        tagcols['arg6'].append(tags[15])\n",
    "                        tagcols['arg7'].append(tags[16])\n",
    "                        tagcols['arg8'].append(tags[17])\n",
    "                        tagcols['arg9'].append(tags[18])\n",
    "                        tagcols['arg10'].append(tags[19])\n",
    "                        \n",
    "                        texts += (\" \" + tags[1].lower())\n",
    "\n",
    "                    i += 1\n",
    "                temp_text_tags[\"text\"] = texts.strip()\n",
    "                tagcols['part'] = part_to_text(tagcols['part'],temp_text_tags[\"text\"]) # from coref number to the word\n",
    "                temp_text_tags[\"tagcols\"] = tagcols\n",
    "                text_tags.append(temp_text_tags)\n",
    "        else:\n",
    "            i += 1\n",
    "    i += 1\n",
    "\n",
    "    doc_dict = {}\n",
    "    doc_dict['doc_id'] = doc_id\n",
    "    doc_dict['text_tags'] = text_tags\n",
    "    all_text_tag.append(doc_dict)\n",
    "fw = open(\"all_text_tag.json\", \"w\", encoding=\"utf-8\")\n",
    "json.dump(all_text_tag, fw, ensure_ascii=False, indent=4)\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259c45b9",
   "metadata": {},
   "source": [
    "# preprocess tags \n",
    "## make them pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b0c8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "    import re\n",
    "    regex = re.compile('[^a-zA-Z]') # to remove .1.2.3 and _ in hidden tags\n",
    "\n",
    "    def useful_tag_8(tags: str):\n",
    "        tags = tags.lower()\n",
    "        if \"|\" in tags:\n",
    "            tags = tags.split(\"|\")\n",
    "            ans = []\n",
    "            for tag in tags:\n",
    "                if tag:\n",
    "                    name, tag = tag.split(\"=\")\n",
    "                    taglist = tag.split(\":\")\n",
    "                    regextaglist = [regex.sub(' ', _) for _ in taglist]\n",
    "                    \n",
    "                    alltag = \" \".join(regextaglist)\n",
    "                    ans.append(name + \" : \" + alltag)\n",
    "                    # replace multiple blankspace with one\n",
    "                    newans = [' '.join(_.split()) for _ in ans] \n",
    "            if len(newans) != 0:\n",
    "                return newans\n",
    "            else:\n",
    "                return \"_\"\n",
    "        \n",
    "        else:\n",
    "            ans = []\n",
    "            if tags != '_':\n",
    "                \n",
    "                name, tag = tags.split(\"=\")\n",
    "                taglist = tag.split(\":\")\n",
    "                regextaglist = [regex.sub(' ', _) for _ in taglist]\n",
    "               \n",
    "                alltag = \" \".join(regextaglist)\n",
    "                ans.append(name + \" : \" + alltag)\n",
    "                newans = [' '.join(_.split()) for _ in ans]\n",
    "                if len(newans) != 0:\n",
    "                    return newans\n",
    "            else:\n",
    "                return \"_\"\n",
    "\n",
    "    def part_to_text(partlist:list,text:str): # change part number to the word\n",
    "        text_list = text.split()\n",
    "        for i, _ in enumerate(partlist):\n",
    "            if _ != '_':\n",
    "                partlist[i] = text_list[int(_)-1]\n",
    "        return partlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0928cf",
   "metadata": {},
   "source": [
    "# merge text and tag\n",
    "## in the form: ' text text # TAG = tag # # TAG = tag # text text '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b97ae550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def mergeTextTag(columns:tuple, jsonFile):\n",
    "    \n",
    "    '''\n",
    "    merge text and tags from json file in the form:    \n",
    "    text text # TAG = tag # # TAG = tag # text text .\n",
    "    \n",
    "    parameters: \n",
    "        columns: tuple of str, where the tags are from, tags can from multiple columns\n",
    "        json: str, the input json file address\n",
    "    \n",
    "    input: \n",
    "        column idx in tuple\n",
    "        json data file\n",
    "        \n",
    "    '''\n",
    "    file = json.load(jsonFile)\n",
    "    paddeddata = []\n",
    "    \n",
    "    for each_doc in file:\n",
    "        paddeddoc = {}\n",
    "        paddeddoc['text']=[]\n",
    "        paddeddoc['doc_id']=each_doc['doc_id'].strip()\n",
    "        \n",
    "        texttag_list = each_doc['text_tags'] # list\n",
    "        \n",
    "        # allign each word with target columns tags of it\n",
    "        for _ in texttag_list: \n",
    "            tags = _['tagcols'] \n",
    "            text_list = _['text'].split(' ') # the sentence list\n",
    "            \n",
    "            # combine cols of tags together in list.\n",
    "            tags_to_merge = [list(t) for t in zip(tags[c] for c in columns)] \n",
    "            unested = [list(itertools.chain(*sub)) for sub in tags_to_merge]\n",
    "            \n",
    "            # unzip the hidden tag list\n",
    "            for l in unested:\n",
    "                for i, v in enumerate(l):\n",
    "                    if type(v) == list:\n",
    "                        l[i] = ', '.join(v)\n",
    "            \n",
    "            # add '#' at the start and end of tags \n",
    "            newtag = []\n",
    "            for idx, _ in enumerate(zip(*unested)): # add tag name and '#' for not meaningful tags\n",
    "                if not all( l == '_' for l in _ ):\n",
    "                    hashpadlist =['# ' + columns[i] + ' : ' + t + ' # ' \\\n",
    "                                for i, t in enumerate(_) if t != '_']\n",
    "                    hashpad = ' '.join(hashpadlist)\n",
    "                    newtag.append(hashpad)\n",
    "                else:\n",
    "                    newtag.append('')\n",
    "            \n",
    "            paddedtext = ''\n",
    "            # and merge with the text list \n",
    "            \n",
    "            for text, tag in zip(text_list, newtag):\n",
    "                paddedtext += text + ' ' + tag\n",
    "            pt = paddedtext.replace('hidden : ','')\n",
    "            # append padded text to the list\n",
    "            paddeddoc['text'].append(pt)\n",
    "        paddeddata.append(paddeddoc)\n",
    "    return paddeddata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94dea941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "jsonpath = r'/Users/rickzhai/Documents/GitHub/ITNLP_Semeval2022_Task6/number_reasoning/all_text_tag.json'\n",
    "f = open(jsonpath, 'r', encoding='utf-8')\n",
    "padded = mergeTextTag(columns=('part','hidden'), jsonFile=f)\n",
    "w = open(\"padded_text_0_1.json\", \"w\", encoding=\"utf-8\")\n",
    "json.dump(padded, w, ensure_ascii=False, indent=4)\n",
    "w.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e35a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
