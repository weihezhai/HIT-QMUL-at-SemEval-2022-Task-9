{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b85b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from collections import namedtuple\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AlbertTokenizer, AlbertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6630ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机数种子\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "setup_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1824ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "def get_premodel(path=r\"D:\\Anaconda\\learn\\_Bert\\pre_train_model\\albert-base-v2\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "    model = AlbertForQuestionAnswering.from_pretrained(path) \n",
    "    print(\"model load end!\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0b26bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(path=r'EM_all_data.json' ,fold=0):\n",
    "    f = open(path, 'r', encoding='utf-8')\n",
    "    json_data = json.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    datas=[]\n",
    "    for item in json_data:\n",
    "        qa,texts = item[\"qa\"],item[\"text\"]\n",
    "        qa['quest'] = qa['quest'].split(\"=\")[1].strip().lower()\n",
    "        qa['answer'] = qa['answer'].strip().lower()\n",
    "        text=\"\"\n",
    "        for tt in texts:\n",
    "            text+=tt[\"text\"]\n",
    "        text = text.strip().lower()\n",
    "        temp_dict={}\n",
    "        temp_dict['qa']=qa\n",
    "        temp_dict['text']=text\n",
    "        datas.append(temp_dict)\n",
    "        \n",
    "    random.shuffle(datas)    \n",
    "    num_data = len(datas)\n",
    "    num_test = num_data // 5\n",
    "    test = datas[fold * num_test:(fold + 1) * num_test]\n",
    "    if fold == 0:\n",
    "        train = datas[num_test:]\n",
    "    else:\n",
    "        train = datas[:num_test * fold]\n",
    "        train.extend(datas[num_test * (fold + 1):])\n",
    "    print(\"split data end !\")\n",
    "\n",
    "    print(\"nums of train data:{}\".format(len(train)))\n",
    "    print(\"nums of val data:{}\".format(len(test)))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "912d5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):  # 需要继承data.Dataset\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        qa=item[\"qa\"]\n",
    "        text=item[\"text\"]\n",
    "        q,a=qa[\"quest\"],qa[\"answer\"]\n",
    "        encode = self.tokenizer.encode_plus(q, text, add_special_tokens=True,\n",
    "                                            max_length=512, \n",
    "                                            padding='max_length',\n",
    "                                            return_attention_mask=True, \n",
    "                                            return_tensors='pt',\n",
    "                                            truncation=True)\n",
    "        input_ids,token_type_ids,  attention_mask = encode['input_ids'],encode['token_type_ids'],  encode['attention_mask']\n",
    "        # 获取起始位置\n",
    "        start, end = self.start_end(a, q, text)\n",
    "        return input_ids.squeeze(), token_type_ids.squeeze(), attention_mask.squeeze(), torch.tensor(start), torch.tensor(end),a\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "                \n",
    "    def start_end(self, answer, q, text):\n",
    "        # 有问题\n",
    "        answer_encode = self.tokenizer(answer.strip().lower())['input_ids'][1:-1]\n",
    "        text_encode = self.tokenizer(q.strip().lower(), text.strip().lower())['input_ids']\n",
    "        start_end = ()\n",
    "        for i in range(len(text_encode)):\n",
    "            if text_encode[i] == answer_encode[0]:\n",
    "                j = 0\n",
    "                for j in range(len(answer_encode)):\n",
    "                    if text_encode[i + j] != answer_encode[j]:\n",
    "                        break\n",
    "                if j == len(answer_encode) - 1:\n",
    "                    if text_encode[i + j] != answer_encode[j]:\n",
    "                        continue\n",
    "                    start_end = (i, i + len(answer_encode) - 1)\n",
    "            if len(start_end) != 0:\n",
    "                return start_end\n",
    "        if len(start_end) == 0:\n",
    "            start_end = (0, 0)\n",
    "        return start_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ff600df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, albert, QAhead):\n",
    "        super().__init__()\n",
    "        self.albert = albert\n",
    "        self.qa_outputs = QAhead  # self.qa_outputs = nn.Linear(768, 2,bias=True)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, start_positions=None, end_positions=None):\n",
    "        albert_output = self.albert(input_ids=input_ids,\n",
    "                                    token_type_ids=token_type_ids,\n",
    "                                    attention_mask=attention_mask)\n",
    "        last_hidden_state = albert_output.last_hidden_state  # torch.Size([batchSize, 512, 768]) 512与max_length相关\n",
    "        logits = self.qa_outputs(last_hidden_state)  # torch.Size([batchSize, 512, 2])\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)  # 分离出的start_logits/end_logits形状为([batchSize, 512, 1])\n",
    "        start_logits = start_logits.squeeze(-1)  # ([batchSize, 512])\n",
    "        end_logits = end_logits.squeeze(-1)  # ([batchSize, 512])\n",
    "\n",
    "        Outputs = namedtuple('Outputs', 'start_logits, end_logits')\n",
    "        outputs = Outputs(start_logits, end_logits)\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "            Outputs = namedtuple('Outputs', 'loss,start_logits, end_logits')\n",
    "            outputs = Outputs(total_loss, start_logits, end_logits)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d25a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,tokenizer, train_dataloader, testdataloader, device, fold=0, epoch=5):\n",
    "    model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    optim = AdamW(model.parameters(), lr=1e-5, weight_decay=0.2)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optim, num_warmup_steps=300, num_training_steps=len(train_dataloader) * epoch)\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        model.train()\n",
    "        train_acc = []\n",
    "        train_loss = []\n",
    "        loop = tqdm(train_dataloader, leave=True)\n",
    "        for data in tqdm(loop):\n",
    "            optim.zero_grad()\n",
    "            a = data[-1][0]\n",
    "            data = tuple(t.to(device) for t in data[:-1])\n",
    "            input_ids, token_type_ids, attention_mask, start, end = data\n",
    "            \n",
    "            outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
    "                            start_positions=start, end_positions=end)\n",
    "            loss, start_logits, end_logits = outputs.loss, outputs.start_logits, outputs.end_logits\n",
    "            loss=loss.mean()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            start_pred = torch.argmax(start_logits, dim=1)\n",
    "            end_pred = torch.argmax(end_logits, dim=1)\n",
    "            starts = (np.array((start_pred == start).cpu()))\n",
    "            ends = (np.array((end_pred == end).cpu()))\n",
    "            acc = np.all(np.array([starts, ends]).T, axis=-1).astype(int)\n",
    "            train_acc.extend(acc)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            loop.set_description(f'fold:{fold}  Epoch:{epoch}')\n",
    "            loop.set_postfix(loss=loss.item(), acc=acc)\n",
    "        # if epoch >=3:\n",
    "        #     model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        #     model_path = r\"fold\" + str(fold) + \"_epoch\" + str(epoch)\n",
    "        #     if not os.path.exists(model_path):\n",
    "        #         os.makedirs(model_path)\n",
    "        #     torch.save(model_to_save, os.path.join(model_path,'model.pt'))\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        test_acc = []\n",
    "        for data in tqdm(testdataloader):\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                a = data[-1][0]\n",
    "\n",
    "                data = tuple(t.to(device) for t in data[:-1])\n",
    "                input_ids, token_type_ids, attention_mask, start, end = data\n",
    "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
    "                                start_positions=start, end_positions=end)\n",
    "\n",
    "                loss = outputs.loss.mean()\n",
    "                start_pred = torch.argmax(outputs.start_logits, dim=1)\n",
    "                end_pred = torch.argmax(outputs.end_logits, dim=1)\n",
    "                pred_ans = (tokenizer.decode(input_ids[0][start_pred:end_pred+1]))\n",
    "                if a==pred_ans:\n",
    "                    test_acc.append(1)\n",
    "                else:\n",
    "                    test_acc.append(0)\n",
    "                test_loss.append(loss.item())\n",
    "        print(\"{}, Train_acc:{} Train_loss:{}-----Val_acc:{}  Val_loss:{}\".format(\n",
    "            epoch, np.mean(train_acc), np.mean(train_loss), np.mean(test_acc),\n",
    "            np.mean(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1aa13a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:\\Anaconda\\learn\\_Bert\\pre_train_model\\albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias']\n",
      "- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at D:\\Anaconda\\learn\\_Bert\\pre_train_model\\albert-base-v2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model load end!\n",
      "split data end !\n",
      "nums of train data:6588\n",
      "nums of val data:1647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/3294 [00:00<?, ?it/s]\n",
      "fold:0  Epoch:0:   0%|                                                  | 0/3294 [00:08<?, ?it/s, acc=[0 0], loss=6.67]\u001b[A\n",
      "fold:0  Epoch:0:   0%|                                       | 1/3294 [00:14<13:36:16, 14.87s/it, acc=[0 0], loss=6.67]\u001b[A\n",
      "  0%|                                                                              | 1/3294 [00:14<13:36:09, 14.87s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10800/2090277115.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_Dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_Dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10800/1576287837.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, tokenizer, train_dataloader, testdataloader, device, fold, epoch)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_logits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold in range(1):\n",
    "    model, tokenizer = get_premodel()\n",
    "    with open(r'E:\\RecipeQA\\data\\ITNLP_Semeval2022_Task6\\baseline_10_28\\QAhead_base.pickle', 'rb') as file:\n",
    "        QAhead = pickle.load(file)\n",
    "    myModel = MyModel(model.albert, QAhead)\n",
    "    \n",
    "    train_data, test_data = split_data()\n",
    "    \n",
    "    # 构造DataSet和DataLoader\n",
    "    train_Dataset = myDataset(train_data, tokenizer)\n",
    "    test_Dataset = myDataset(test_data, tokenizer)\n",
    "    # 修改batchsize\n",
    "    train_Dataloader = DataLoader(train_Dataset, batch_size=2, shuffle=True)\n",
    "    test_Dataloader = DataLoader(test_Dataset, batch_size=1)\n",
    "    # 训练\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = torch.device('cpu')\n",
    "    train(myModel, tokenizer,train_Dataloader, test_Dataloader, device, fold, epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c1240e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:\\Anaconda\\learn\\_Bert\\pre_train_model\\albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias']\n",
      "- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at D:\\Anaconda\\learn\\_Bert\\pre_train_model\\albert-base-v2 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model load end!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_premodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5bd7c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    " model.save_pretrained(r\"C:\\Users\\fengmq\\Desktop\\tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43873916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
