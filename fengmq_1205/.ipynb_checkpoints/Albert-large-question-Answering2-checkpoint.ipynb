{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23bd233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from collections import namedtuple\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AlbertTokenizer, AlbertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e86b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机数种子\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "setup_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd43ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "def get_premodel(path=r\"D:\\Anaconda\\learn\\_Bert\\pre_train_model\\albert-base-v2\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "    model = AlbertForQuestionAnswering.from_pretrained(path) \n",
    "    print(\"model load end!\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1abee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(path=r'EM_all_data.json' ,fold=0):\n",
    "    f = open(path, 'r', encoding='utf-8')\n",
    "    json_data = json.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    datas=[]\n",
    "    for item in json_data:\n",
    "        qa,texts = item[\"qa\"],item[\"text\"]\n",
    "        qa['quest'] = qa['quest'].split(\"=\")[1].strip().lower()\n",
    "        qa['answer'] = qa['answer'].strip().lower()\n",
    "        text=\"\"\n",
    "        for tt in texts:\n",
    "            text+=tt[\"text\"]\n",
    "        text = text.strip().lower()\n",
    "        temp_dict={}\n",
    "        temp_dict['qa']=qa\n",
    "        temp_dict['text']=text\n",
    "        datas.append(temp_dict)\n",
    "        \n",
    "    random.shuffle(datas)    \n",
    "    num_data = len(datas)\n",
    "    num_test = num_data // 5\n",
    "    test = datas[fold * num_test:(fold + 1) * num_test]\n",
    "    if fold == 0:\n",
    "        train = datas[num_test:]\n",
    "    else:\n",
    "        train = datas[:num_test * fold]\n",
    "        train.extend(datas[num_test * (fold + 1):])\n",
    "    print(\"split data end !\")\n",
    "\n",
    "    print(\"nums of train data:{}\".format(len(train)))\n",
    "    print(\"nums of val data:{}\".format(len(test)))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):  # 需要继承data.Dataset\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        qa=item[\"qa\"]\n",
    "        text=item[\"text\"]\n",
    "        q,a=qa[\"quest\"],qa[\"answer\"]\n",
    "        encode = self.tokenizer.encode_plus(q, text, add_special_tokens=True,\n",
    "                                            max_length=512, \n",
    "                                            padding='max_length',\n",
    "                                            return_attention_mask=True, \n",
    "                                            return_tensors='pt',\n",
    "                                            truncation=True)\n",
    "        input_ids,token_type_ids,  attention_mask = encode['input_ids'],encode['token_type_ids'],  encode['attention_mask']\n",
    "        # 获取起始位置\n",
    "        start, end = self.start_end(a, q, text)\n",
    "        return input_ids.squeeze(), token_type_ids.squeeze(), attention_mask.squeeze(), torch.tensor(start), torch.tensor(end)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "                \n",
    "    def start_end(self, answer, q, text):\n",
    "        # 有问题\n",
    "        answer_encode = self.tokenizer(answer.strip().lower())['input_ids'][1:-1]\n",
    "        text_encode = self.tokenizer(q.strip().lower(), text.strip().lower())['input_ids']\n",
    "        start_end = ()\n",
    "        for i in range(len(text_encode)):\n",
    "            if text_encode[i] == answer_encode[0]:\n",
    "                j = 0\n",
    "                for j in range(len(answer_encode)):\n",
    "                    if text_encode[i + j] != answer_encode[j]:\n",
    "                        break\n",
    "                if j == len(answer_encode) - 1:\n",
    "                    if text_encode[i + j] != answer_encode[j]:\n",
    "                        continue\n",
    "                    start_end = (i, i + len(answer_encode) - 1)\n",
    "            if len(start_end) != 0:\n",
    "                return start_end\n",
    "        if len(start_end) == 0:\n",
    "            start_end = (0, 0)\n",
    "        return start_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, albert, QAhead):\n",
    "        super().__init__()\n",
    "        self.albert = albert\n",
    "        self.qa_outputs = QAhead  # self.qa_outputs = nn.Linear(768, 2,bias=True)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, start_positions=None, end_positions=None):\n",
    "        albert_output = self.albert(input_ids=input_ids,\n",
    "                                    token_type_ids=token_type_ids,\n",
    "                                    attention_mask=attention_mask)\n",
    "        last_hidden_state = albert_output.last_hidden_state  # torch.Size([batchSize, 512, 768]) 512与max_length相关\n",
    "        logits = self.qa_outputs(last_hidden_state)  # torch.Size([batchSize, 512, 2])\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)  # 分离出的start_logits/end_logits形状为([batchSize, 512, 1])\n",
    "        start_logits = start_logits.squeeze(-1)  # ([batchSize, 512])\n",
    "        end_logits = end_logits.squeeze(-1)  # ([batchSize, 512])\n",
    "\n",
    "        Outputs = namedtuple('Outputs', 'start_logits, end_logits')\n",
    "        outputs = Outputs(start_logits, end_logits)\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "            Outputs = namedtuple('Outputs', 'loss,start_logits, end_logits')\n",
    "            outputs = Outputs(total_loss, start_logits, end_logits)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,tokenizer, train_dataloader, testdataloader, device, fold=0, epoch=5):\n",
    "    model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    optim = AdamW(model.parameters(), lr=1e-5, weight_decay=0.2)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optim, num_warmup_steps=300, num_training_steps=len(train_dataloader) * epoch)\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        model.train()\n",
    "        train_acc = []\n",
    "        train_loss = []\n",
    "        loop = tqdm(train_dataloader, leave=True)\n",
    "        for data in tqdm(loop):\n",
    "            optim.zero_grad()\n",
    "            a = data[-1][0]\n",
    "            data = tuple(t.to(device) for t in data[:-1])\n",
    "            input_ids, token_type_ids, attention_mask, start, end = data\n",
    "            \n",
    "            outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
    "                            start_positions=start, end_positions=end)\n",
    "            loss, start_logits, end_logits = outputs.loss, outputs.start_logits, outputs.end_logits\n",
    "            loss=loss.mean()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            start_pred = torch.argmax(start_logits, dim=1)\n",
    "            end_pred = torch.argmax(end_logits, dim=1)\n",
    "            starts = (np.array((start_pred == start).cpu()))\n",
    "            ends = (np.array((end_pred == end).cpu()))\n",
    "            acc = np.all(np.array([starts, ends]).T, axis=-1).astype(int)\n",
    "            train_acc.extend(acc)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            loop.set_description(f'fold:{fold}  Epoch:{epoch}')\n",
    "            loop.set_postfix(loss=loss.item(), acc=acc)\n",
    "        # if epoch >=3:\n",
    "        #     model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        #     model_path = r\"fold\" + str(fold) + \"_epoch\" + str(epoch)\n",
    "        #     if not os.path.exists(model_path):\n",
    "        #         os.makedirs(model_path)\n",
    "        #     torch.save(model_to_save, os.path.join(model_path,'model.pt'))\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        test_acc = []\n",
    "        for data in tqdm(testdataloader):\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                a = data[-1][0]\n",
    "\n",
    "                data = tuple(t.to(device) for t in data[:-1])\n",
    "                input_ids, token_type_ids, attention_mask, start, end = data\n",
    "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
    "                                start_positions=start, end_positions=end)\n",
    "\n",
    "                loss = outputs.loss.mean()\n",
    "                start_pred = torch.argmax(outputs.start_logits, dim=1)\n",
    "                end_pred = torch.argmax(outputs.end_logits, dim=1)\n",
    "                pred_ans = (tokenizer.decode(input_ids[0][start_pred:end_pred+1]))\n",
    "                if a==pred_ans:\n",
    "                    test_acc.append(1)\n",
    "                else:\n",
    "                    test_acc.append(0)\n",
    "                test_loss.append(loss.item())\n",
    "        print(\"{}, Train_acc:{} Train_loss:{}-----Val_acc:{}  Val_loss:{}\".format(\n",
    "            epoch, np.mean(train_acc), np.mean(train_loss), np.mean(test_acc),\n",
    "            np.mean(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c6c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(1):\n",
    "    model, tokenizer = get_premodel()\n",
    "    with open(r'QAhead_large.pickle', 'rb') as file:\n",
    "        QAhead = pickle.load(file)\n",
    "    myModel = MyModel(model.albert, QAhead)\n",
    "    \n",
    "    train_data, test_data = split_data()\n",
    "    \n",
    "    # 构造DataSet和DataLoader\n",
    "    train_Dataset = myDataset(train_data, tokenizer)\n",
    "    test_Dataset = myDataset(test_data, tokenizer)\n",
    "    # 修改batchsize\n",
    "    train_Dataloader = DataLoader(train_Dataset, batch_size=2, shuffle=True)\n",
    "    test_Dataloader = DataLoader(test_Dataset, batch_size=1)\n",
    "    # 训练\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = torch.device('cpu')\n",
    "    train(myModel, tokenizer,train_Dataloader, test_Dataloader, device, fold, epoch=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
